---
title: "MSMB Chapter 1 Review: Simulating Random Events"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Biology Context

Let's start with the example problem from Modern Statistics for Modern Biology Section 1.2. An HIV virus has a per-base-pair mutation rate of $5*10^{-4}$ mutations/replication. We start by asking if we can simulate whether or not one particular base pair will mutate during a given replication. 

If each base pair has a probability of $5*10^{-4}$ to mutate, one intuitive way to do this would be to select a random number between 0 and 1, and if the number is between 0 and $5*10^{-4}$, say the base pair mutated, and otherwise say no. 

```{r Single Base Pair,eval=FALSE}
mutation_prob<- #Assigning the mutation rate
## We simulate using the r function runif to call a Random number from a UNIForm distribution
decision_number<- #we only want 1 number
mutation<- #Did a mutation occur?
print(mutation)
```

What we just did was a $\textit{Bernoulli Trial}$, which is just the statistics word for flipping a biased coin. In our simulation, we can say a mutation is heads and no mutation is tails and the probability of the coin landing on heads is 0.05%. If we assume, like a coin flip, each trial is independent of the other trials, we can just do them over and over again to simulate many random events. For example, if we assume each nucleotide in the HIV genome undergoes substitution mutations independently of the other base pairs (which yeah yeah but for the sake of learning), then we can say the total number of base pair mutations in one replication is the sum of (number of bp) Bernoulli Trials. 

```{r Multiple Base Pairs Part 1,eval=FALSE}
## What about a whole genome's worth? How many would we expect? How variable would we expect this rate to be?
## Let's simulate to find out
## First, for the sake of everyone getting the same answer we can set the random number generator.
set.seed(1297)
decision_numbers<- #R tip: the first argument for runif is the how many random numbers you want, so if you set a large number you get back a vector of numbers
total_mutations<- #R tip: R encodes FALSE=0 and TRUE=1 for the purpose of converting logical symbols to numbers. So sum({logical vector}) will give you the number of TRUEs. Also, the logical operators >,<,!=,== etc can automatically tell when you're comparing a single numeric to a vector and will do the comparison on the entire vector at once. Caution: If you compare two vectors of different lengths to each other, R will do some weird stuff, so avoid doing that. 
print(total_mutations)
## Pretty close to our expectation no?
```
## Doing a bunch of Bernoulli Trials Over and Over is Exhausting
Don't worry, I agree. Fortunately, so did statisticians a long time ago. In fact, they characterized a function for determining the probability of $p$ successes in $n$ Bernoulli Trials as a $\textit{Binomial Distribution}$ written $Bin(n,p)$. R has a built in function to simulate numbers of successes according to a binomial distribution - rbinom. Special note: This distribution in particular assumes that we do not care about the order of the successes and failures, just the total number. 

```{r Multiple Base Pairs Part 2,eval=FALSE}
## Now we can quickly sample the number of mutations for a genome replication
number_mutations<- ## The n argument here tells us how many random numbers we want to generate, so this time we say 1 for 1 replication. 
print(number_mutations)
```
## What if we want to look at what happens over many (independent) genome replications?

Based on what we've built up so far, we have a computational strategy for simulating (1) whether or not any give base pair mutates during one replication (Bernoulli Trial); (2) how many mutations occur over an entire genome during one replication (Sum of independent Bernoulli Trials or Binomial random number). 
What if we want to know something about the distribution of the sum of genome-wide mutations during the replication process? We've learned now that we expect for there to be about 5 mutations per replication, but how likely is 5 compared to 4 or 6? 10? Let's do some simulations to find out.

R has 3 ways (I can immediately think of) to perform this simulation, so we're going to try all of them and see if we get the same result from every method. First, we're going to rely on our good old Bernoulli trials. If the number of mutations in one genome replication is the sum of Bernoulli trials equal to the number of base pairs in the genome, why not just do that but many times? To make the code a little tidier we're going to use the apply R function. 
```{r Multiple Genome Replications,eval=FALSE}
## This was for one replication for one genome. To get a feeling for the variance, we can simulate a bunch of times
## We're going to make this fast by using the apply family of functions
n_genomes<- #Say we're going to look at 5000 genomes replicating
## Now we're going to store the information from our simulation of each genome in a matrix
## For this matrix, the columns will each represent 1 genome, and the rows will each represent 
## 1 base pair. The number (simulated by runif) will represent the Bernoulli trial conducted on
## that base pair for that genome. If the value is <= mutation rate, we call that a mutation.
decision_matrix<-
## Let's see what this matrix looks like
str(decision_matrix)
## To operate on all columns of this matrix at once, we'll use apply.
## In R, the apply family of functions is there so that you don't have to worry about
## writing your own for loops to perform a function many times. apply works specifically on
## matrices. The function goes like this:
## apply(matrix, (1 for apply a function to every row, 2 for every column), function)
## In this implementation we're using an anonymous function, which means that we only define it
## in the command where we use it, so it isn't stored in memory. I do this a lot for my own convenience
## when I want to do an operation that I'm only ever going to want to do once and don't want to bother
## writing out the proper function for it, here I'm just demonstrating. 
## The syntax of an anonymous function is the same as the syntax of a regular function
## function(variables) {what to return} So in this case our function takes input x
## where x is a column of decision_matrix and gives me the number of TRUE's for x<=mutation_prob
n_mutations<-
## Let's see what we got
## First we can check out the format of the output
str(n_mutations)
## We get one number for each column
hist(n_mutations,main='Number Mutations per Replication')
## Now let's learn a little bit about this simulations
avg_number_mutations<-
var_number_mutations<-

print(avg_number_mutations)
print(var_number_mutations)
## Interesting... the mean and the variance appear to be the same. And remember, std dev = sqrt(var)
## so the standard deviation of this distribution would be like sqrt(5)~2.236
```
## Can we do this faster?

Spoiler alert: Yes. While we had to go through the entire rigmarole of simulating each individual base pair for each individual replication with Bernoulli trials, we already know that by selecting binomial random numbers we can simulate one entire genome replication all at once. So let's save ourselves the time and use rbinom.

```{r Binomial Simulation,eval=FALSE}
## This is gonna be so much easier
mutation_binomial<-
## Boom
hist(mutation_binomial,xlab='n_mutations',main='Histogram of Binomial Simulations')
## Now let's learn something about the distribution
binomial_mean<-
binomial_var<-
print(binomial_mean)
print(binomial_var)

## Ooh look at that very similar to our Bernoulli trial-based approach. 
## Note again that mean~variance. Also, note that this distribution is *asymmetric* looking.

```
## Something I've Been Hiding
We can take a brief second to throw in some math notation and talk about what's going on under the hood here. We are able to conduct these simulations and generate random numbers with a probability distribution, which is characterized by a $\textit{Probability Mass Function (PMF)}$. For a Bernoulli Trial, the PMF is pretty simple. If we assign probability $p$ to a mutation and probability $1-p$ to no mutation, then we say:

$P(X=mutation)=p, P(X=no~mutation)=1-p$.

The PMF of a Binomial distribution is the product of $n$ independent Bernoulli trials. So we can guess the PMF for $k$ mutations in $n$ base pairs will look something like:

$P(X=k)=p^k(1-p)^{n-k}$

BUT. Like I cautioned earlier, we don't care about the order in this case, so there are many possible ways to get $k$ successes in $n$ trials, so we need to add the probabilities of all of those together. In fact, the number of ways can be explained by a fancy operator called $\textit{choose}$, which we write like

$P(X=k)={n\choose k}p^k(1-p)^{n-k}$

You can look up the exact formula for how it's calculated, but R has function choose(n,k) that allows you to not. The fact of the matter, however, is that it's a pain to compute for large numbers because it involves factorials. So what happens when the number of trials is very large? VERY LARGE. Like approaches infinity large. We can do some derivations if we want but the story is in the case $n>>p$, the factorial simplifies to an exponential and we land at a PMF that looks like:

$P(X=k)=\frac{(np)^k}{k!}e^{-np}$

This limit is called the $\textit{Poisson Distribution}$ and it a hugely important result in biological mathematics, statistics, and dynamical systems! It turns out that in the real world, the $n>>p$ limit generally holds and this distribution gives us a lot of powerful tools to analyze counts of things (like counts of reads, counts of mutations, counts of cells). Using the Poisson distribution, we can perform linear regression on count data where the assumption of normally distributed errors is violated. Additionally, if we define the quantity $np$ to be one constant ($\lambda$), then we only have to estimate 1 parameter to understand the entire distribution. 
```{r Comparing Theory to Simulations,eval=FALSE}
## Simulating with Poisson distribution
mutations_pois<-
## The syntax here is the same as the binomial, except this time we only provide one parameter
## lambda, which in the binomial framework is n*p. 
hist(n_mutations,main='Bernoulli Simulation + Poisson Density',freq=FALSE)
lines(0:15,dpois(0:15,lambda=5),col='blue',lwd=2)
legend(legend=c('Simulation','Theory'),fill=c('black','blue'),
       x=8,y=0.15)

## Let's do a slightly more sophisticated visualization
## Using library of all libraries, ggplot2
library(ggplot2)
## Before we can do ggplots, we need to transform the data into a data.frame
method_labels<-rep(c('Bernoulli','Binomial','Poisson'),each=???)
plotting_data_frame<-data.frame(method=???,
                                result=???)
## Constructing out plot
sim_plot<-ggplot(plotting_data_frame)+
  geom_bar(aes(x=result,fill=method,group=method),position='dodge')
sim_plot

## This is a good start but we can jazz this up to be much more readable
sim_plot_nicer<-sim_plot+
  facet_wrap(~method,ncol=1)+
  xlab('Number Mutations')+
  ylab('Frequency')+
  scale_fill_brewer(name='Simulation Method',palette='Set2')+
  theme_bw()+
  theme(strip.background=element_blank(),
        text=element_text(size=18))
sim_plot_nicer

## And now we can compare our simulation schemes and see that they all follow from each other, cool!
library(dplyr)
summary_info<-plotting_data_frame %>%
  group_by(???) %>%
  summarize(mean_val=???,
            var_val=???,
            sd_val=???) %>%
  mutate(x_pos=13,
         y_pos=750)
sim_plot_with_additional_info<-sim_plot_nicer+
  geom_text(data=summary_info,
            aes(x=x_pos,y=y_pos,label=paste0('Mean: ',mean_val)),
            size=4)+
  geom_text(data=summary_info,
            aes(x=x_pos,y=y_pos-200,label=paste0('Var: ',round(var_val,4))),
            size=4)
sim_plot_with_additional_info
  


```

## The Poisson Distribution's More Data-Friendly Partner, the Negative Binomial
While the Poisson distribution is extremely powerful and gives us the ability to do a lot of really great statistics, it has one drawback that the mean is always equal to the variance. However, we often find in real count data (such as sequencing data), that the higher the mean, the higher the variance. This relationship is called $\textit{Heteroscedasticity}$. And heteroscedastic data sometimes can break Poisson distribution-based methods. To counter this, we have a generalization of the Poisson distribution called the $\textit{Negative Binomial}$ distribution. 

Let's talk through an example of what the negative binomial simulates. If I have a coin, I may want to know how many flips I can do before I get heads three times. The negative binomial distribution will model the probability that my third heads is the third coin flip, fourth coin flip, etc... 

So let's use our HIV genome as an example. Let's say I know the mutation rate is 5e-4 mutations/base pair replication, and I know from one replication to another a genome acquires 5 mutations. What is the probability the genome is around 1e4 base pairs long?

```{r Demoing Negative Binomial Distribution,eval=FALSE}
## First to get comfortable, let's sample from this distribution
neg_bin_samples<-
hist(neg_bin_samples,main='Sampling Negative Binomial Distribution',xlab='Genome Length')
## Let's see how often we got 1e4 +/- 10% as our genome length
number_within_tenpercent<-
density_within_tenpercent<-
print(density_within_tenpercent)
## About 17% of our samples were within ten percent of our expectation. 
## What would the theoretical distribution say?
theor_density<-
print(theor_density)
## Theory says about 17% not bad.
```
## Congratulations!

Now that you're comfortable with the Negative Binomial distribution you are well on your way to writing your own version of popular tools such as DESeq2 and edgeR (which are pretty much negative binomial regression)! 

To review, today we've started from how to simulate a biased coin flip (Bernoulli trial), to how to simulate the sum of many independent flips (drawing from a binomial distribution), to how to simulate the sum of infinitely many flips (drawing from a Poisson distribution), to how to predict how many coin flips were performed if we know the number of times we got heads and the probability of getting heads (drawing from a negative binomial distribution). We've also covered a little the apply family of functions in R, base R data visualization with hist(), more sophisticated publication-ready visualization with ggplot2, and some basics of data frame manipulation and summary with dplyr. Good luck and good coding!
